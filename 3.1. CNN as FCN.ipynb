{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to express CNN as FCN with some weight binding and fixed weights as shown in https://medium.com/impactai/cnns-from-different-viewpoints-fab7f52d159c.\n",
    "\n",
    "![CNN as matrix multiplication](https://miro.medium.com/max/1400/1*95lL-PY5WEeBAtfaWAIIRQ.png)\n",
    "\n",
    "> The matrix above is a weight matrix, just like the ones from traditional neural networks. However, this weight matrix has two special properties:\n",
    "> 1. The zeros shown in gray are untrainable. This means that they’ll stay zero throughout the optimization process.\n",
    "> 2. Some of the weights are equal, and while they are trainable (i.e. changeable), they must remain equal. These are called “shared weights”.\n",
    "> The zeros correspond to the pixels that the filter didn’t touch. Each row of the weight matrix corresponds to one application of the filter.\n",
    "\n",
    "For sure, this is not an efficient way, but it's a nice thought excersice. Note, that this way we use number of output elements by image size weights, thus if we have 28x28 image and 3x3 filter we will end up with more than 7k params.\n",
    "\n",
    "**Q:** is there a way to express CNN as FCN using less weights?\n",
    "\n",
    "**A:** Let's say initial image is 5x5 and we use 2x2 filter, then output will have 4x4 values. If we think about transformation alone we need to map 5x5 to 4x4 and using FCN most 'efficient' reimplementation would have to use 5x5x4x4 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "X = torch.randn((5, 5), dtype=torch.float32)\n",
    "# Expected\n",
    "Y = torch.conv2d(X[None, None, :, :], F[None, None, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sure this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[a, b],\n",
    " [c, d]] = F\n",
    "\n",
    "A = torch.tensor([[a, b, 0, 0, 0, c, d, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                  [0, a, b, 0, 0, 0, c, d, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                  [0, 0, a, b, 0, 0, 0, c, d, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, a, b, 0, 0, 0, c, d, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0, 0, a, b, 0, 0, 0, c, d, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0, 0, 0, a, b, 0, 0, 0, c, d, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0, 0, 0, 0, a, b, 0, 0, 0, c, d, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0, 0, 0, 0, 0, a, b, 0, 0, 0, c, d, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, a, b, 0, 0, 0, c, d, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, a, b, 0, 0, 0, c, d, 0, 0, 0, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, a, b, 0, 0, 0, c, d, 0, 0, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, a, b, 0, 0, 0, c, d, 0, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, a, b, 0, 0, 0, c, d, 0, 0, 0],\n",
    "                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, a, b, 0, 0, 0, c, d, 0, 0],\n",
    "                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, a, b, 0, 0, 0, c, d, 0],\n",
    "                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, a, b, 0, 0, 0, c, d]])\n",
    "\n",
    "assert torch.allclose(A @ X.flatten(), Y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.43 µs ± 300 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "A @ X.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** is there a way to avoid image reshaping?\n",
    "\n",
    "**A:** Again, let's consider shape transformation from previous answer, we want to map 5x5 to 4x4. We could construct two weight matrices and use one on the left and one on the right 4x5 @ 5x5 @ 5x4 or we can mask part of the initial matrix and express it as 4x5 @ 5x4 + 4x5 @ 5x4.\n",
    "\n",
    "To make our task more interesting, let's to exactly that - produce two weight matrices that could replicate CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[a, b],\n",
    " [c, d]] = F\n",
    "\n",
    "A = torch.tensor([[a, c, 0, 0, 0],\n",
    "                  [0, a, c, 0, 0],\n",
    "                  [0, 0, a, c, 0],\n",
    "                  [0, 0, 0, a, c]])\n",
    "\n",
    "B = torch.tensor([[b, d, 0, 0, 0],\n",
    "                  [0, b, d, 0, 0],\n",
    "                  [0, 0, b, d, 0],\n",
    "                  [0, 0, 0, b, d]])\n",
    "\n",
    "C = A @ X[:, :-1] + B @ X[:, 1:]\n",
    "\n",
    "assert torch.allclose(C, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 µs ± 4.37 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "C = A @ X[:, :-1] + B @ X[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That look's much nicer, but it looks like computation is ~3x slower.\n",
    "\n",
    "Let's instead benchmark for bigger images 256x256 and 3x3 filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn((256, 256), dtype=torch.float32)\n",
    "F = torch.randn((3, 3), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Real conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 µs ± 68.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "torch.conv2d(X[None, None, :, :], F[None, None, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Our flat version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn((254, 256 * 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3 ms ± 299 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "A @ X.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Nicer version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn((254, 256), dtype=torch.float32)\n",
    "B = torch.randn((254, 256), dtype=torch.float32)\n",
    "C = torch.randn((254, 256), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567 µs ± 62.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "A @ X[:, :-2] + B @ X[:, 1:-1] + C @ X[:, 2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it turns out that this nicer variation is quite fast compared to flat one. For sure if filter size grows we end up with a lot of operation here, but usually filter size is small compared to image size and this is not supposed to be normal implementation either way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Layer (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLayer():\n",
    "    def __init__(self, shape, filter_shape):\n",
    "        self.n = filter_shape[0]\n",
    "        self.F = torch.randn(filter_shape)\n",
    "        self.b = torch.zeros((1))\n",
    "        self.out_shape = (shape[0] - filter_shape[0] + 1,\n",
    "                          shape[1] - filter_shape[1] + 1)\n",
    "        self.params = [self.F]\n",
    "        self.inp = None\n",
    "        self.out = None\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.inp = x\n",
    "        self.out = torch.zeros(self.out_shape)\n",
    "        for i in range(self.n):\n",
    "            end_idx = i + 1 - self.n if i + 1 - self.n else None\n",
    "            self.out += self._filt_as_mat(i) @ X[:, i:end_idx]\n",
    "        self.out += self.b\n",
    "        return self.out\n",
    "    \n",
    "    def _filt_as_mat(self, idx):\n",
    "        raise NotImplemented\n",
    "    \n",
    "    def back(self):\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
